{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nick088Official/Face-Fusion-Headless-Colab/blob/main/Face_Fusion_Headless_No_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVreYca3LcQ"
      },
      "source": [
        "# **FACE FUSION NO UI (Manual, Headless Version) 2.3.0**\n",
        "\n",
        "DeepFake AI Tool\n",
        "\n",
        "Google Colab Notebook Made By **Nick088**:\n",
        "- [Youtube](https://www.youtube.com/channel/@Nick088Official)\n",
        "- [TikTok](https://www.tiktok.com/@forgotforever)\n",
        "- [Reddit](reddit.com/user/Nick088Real)\n",
        "- [Twitter](https://twitter.com/Nick088Official)\n",
        "- [Discord](https://discord.com/channels/@me/911742715019001897)\n",
        "\n",
        "## GUIDE:\n",
        "1. Run Install Face Fusion and wait until it finishes.\n",
        "\n",
        "Remember for the rest of the guide:\n",
        "\n",
        "Source = The photo used as a reference point\n",
        "\n",
        "Target = The video or photo whose face you want to change\n",
        "\n",
        "Inputs = Target & Source\n",
        "\n",
        "Output = Final Results (an image if the target is one, or a video if the target is one\n",
        "\n",
        "2. Now you got 2 ways to upload the inputs:\n",
        "\n",
        "A) MANUAL INPUTS WAY:\n",
        "1. On the left of the page click on the folder icon which will show you the Files section, right click on the y8e23w6s folder (Called like that for bypassing the Google Colab ban) and Upload the Source Image and the Target Video/Photo.\n",
        "\n",
        "2. Wait until the Target and Source files are loaded, and then in the Run Face Fusion cell, in Target insert the name of the Target Video/Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), for Source_Image put the name of the Photo Source, and for Output insert the name (with the same extension of the Target) you want to give to the final output.\n",
        "\n",
        "B) GOOGLE DRIVE INPUTS WAY:\n",
        "1. Run the Mount Google Drive Cell\n",
        "\n",
        "2. Goto https://drive.google.com and go into the Face_Fusion_Headless Folder, Upload your Image Source & Target Video/Photo in the Inputs folder inside of it, wait until the Target and Source files are fully loaded in it.\n",
        "\n",
        "3. Check the Google Drive Option in Run Face Fusion Cell, in Target_Video insert the name of the Video or the Target Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), for Source_Image put the name of the photo Source, and for Output insert the name you want to give to the final output (with the same extension of the Target).\n",
        "\n",
        "Now the rest of the guide works for both ways regardless which you used:\n",
        "\n",
        "4. You can put additional options in the Run Face Fusion Cell before running it, all options with explainations:\n",
        "```\n",
        "misc:\n",
        "  --skip-download                                                                                                    omit automate downloads and lookups\n",
        "execution:\n",
        "  --execution-providers EXECUTION_PROVIDERS [EXECUTION_PROVIDERS ...]                                                choose from the available execution providers (choices: cpu, azure)\n",
        "  --execution-thread-count [1-128]                                                                                   specify the number of execution threads\n",
        "  --execution-queue-count [1-32]                                                                                     specify the number of execution queries\n",
        "face selector:\n",
        "  --face-selector-mode {reference,one,many}                                                                          specify the mode for the face selector\n",
        "  --reference-face-position REFERENCE_FACE_POSITION                                                                  specify the position of the reference face\n",
        "  --reference-face-distance [0.0-1.5]                                                                                specify the distance between the reference face and the target face\n",
        "  --reference-frame-number REFERENCE_FRAME_NUMBER                                                                    specify the number of the reference frame\n",
        "face analyser:\n",
        "  --face-analyser-order {left-right,right-left,top-bottom,bottom-top,small-large,large-small,best-worst,worst-best}  specify the order used for the face analyser\n",
        "  --face-analyser-age {child,teen,adult,senior}                                                                      specify the age used for the face analyser\n",
        "  --face-analyser-gender {male,female}                                                                               specify the gender used for the face analyser\n",
        "  --face-detector-model {retinaface,yunet}                                                                           specify the model used for the face detector\n",
        "  --face-detector-size {160x160,320x320,480x480,512x512,640x640,768x768,960x960,1024x1024}                           specify the size threshold used for the face detector\n",
        "  --face-detector-score [0.0-1.0]                                                                                    specify the score threshold used for the face detector\n",
        "face mask:\n",
        "  --face-mask-types FACE_MASK_TYPES [FACE_MASK_TYPES ...]                                                            choose from the available face mask types (choices: box, occlusion, region)\n",
        "  --face-mask-blur [0.0-1.0]                                                                                         specify the blur amount for face mask\n",
        "  --face-mask-padding FACE_MASK_PADDING [FACE_MASK_PADDING ...]                                                      specify the face mask padding (top, right, bottom, left) in percent\n",
        "  --face-mask-regions FACE_MASK_REGIONS [FACE_MASK_REGIONS ...]                                                      choose from the available face mask regions (choices: skin, left-eyebrow, right-eyebrow, left-eye, right-eye, eye-glasses, nose, mouth, upper-lip, lower-lip)\n",
        "frame extraction:\n",
        "  --trim-frame-start TRIM_FRAME_START                                                                                specify the start frame for extraction\n",
        "  --trim-frame-end TRIM_FRAME_END                                                                                    specify the end frame for extraction\n",
        "  --temp-frame-format {jpg,png}                                                                                      specify the image format used for frame extraction\n",
        "  --temp-frame-quality [0-100]                                                                                       specify the image quality used for frame extraction\n",
        "  --keep-temp                                                                                                        keep the temporary resources after processing\n",
        "output creation:\n",
        "  --output-image-quality [0-100]                                                                                     specify the quality used for the output image\n",
        "  --output-video-encoder {libx264,libx265,libvpx-vp9,h264_nvenc,hevc_nvenc}                                          specify the encoder used for the output video\n",
        "  --output-video-preset {ultrafast,superfast,veryfast,faster,fast,medium,slow,slower,veryslow}                       balance fast video processing and video file size\n",
        "  --output-video-quality [0-100]                                                                                     specify the quality used for the output video\n",
        "  --output-video-resolution OUTPUT_VIDEO_RESOLUTION                                                                  specify the video output resolution based on the target video\n",
        "  --output-video-fps OUTPUT_VIDEO_FPS                                                                                specify the video output fps based on the target video\n",
        "  --skip-audio                                                                                                       omit audio from the target\n",
        "frame processors:\n",
        "  --frame-processors FRAME_PROCESSORS [FRAME_PROCESSORS ...]                                                         choose from the available frame processors (choices: face_debugger, face_enhancer, face_swapper, frame_enhancer, ...)\n",
        "  --face-debugger-items FACE_DEBUGGER_ITEMS [FACE_DEBUGGER_ITEMS ...]                                                specify the face debugger items (choices: bbox, kps, face-mask, score)\n",
        "  --face-enhancer-model {codeformer,gfpgan_1.2,gfpgan_1.3,gfpgan_1.4,gpen_bfr_256,gpen_bfr_512,restoreformer}        choose the model for the frame processor\n",
        "  --face-enhancer-blend [0-100]                                                                                      specify the blend amount for the frame processor\n",
        "  --face-swapper-model {blendswap_256,inswapper_128,inswapper_128_fp16,simswap_256,simswap_512_unofficial}           choose the model for the frame processor\n",
        "  --frame-enhancer-model {real_esrgan_x2plus,real_esrgan_x4plus,real_esrnet_x4plus}                                  choose the model for the frame processor\n",
        "  --frame-enhancer-blend [0-100]                                                                                     specify the blend amount for the frame processor\n",
        "  --lip-syncer-model {wav2lip_gan}                                                                                   choose the model responsible for syncing the lips\n",
        "  ```\n",
        "\n",
        "5. Now you got 2 ways to get the outputs:\n",
        "\n",
        "A) MANUAL OUTPUTS WAY:\n",
        "1. After the output is completed, Go to the Files Section .\n",
        "\n",
        "2. Next to the output, click the 3 dots and click download, don't close anything until it finishes downloading.\n",
        "\n",
        "B) GOOGLE DRIVE OUTPUTS WAY:\n",
        "1. After the output is completed, Goto https://drive.google.com.\n",
        "\n",
        "2. Go to the Face_Fusion_Headless Folder, and inside of it, the Outputs Folder, you will find the output here.\n",
        "\n",
        "**TIPS:**\n",
        "- Be careful to make some links every now and then so it doesn't disconnect due to inactivity, you could also check https://rentry.org/colab_workarounds.\n",
        "\n",
        "- If the process is too slow you can speed it up at the cost of losing some quality of the output, by removing “face_enhancer” and “frame_enhancer” in Options of the Run Face Fusion Cell,after frame_processors, before running it again.\n",
        "\n",
        "- If the process is too slow, you can split the Target video into videos of 1 minute each, and run the process again for each video, then finally link them all together by placing them on any free editing app like Capcut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlrnUA3i3gMB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title INSTALL FACE FUSION\n",
        "!git clone http://tinyurl.com/y8e23w6s --branch 2.3.0 --single-branch\n",
        "!pip install onnxruntime-gpu && pip install -r /content/y8e23w6s/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (OPTIONAL) Mount Google Drive\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/drive')\n",
        "!mkdir /content/drive/MyDrive/Face_Fusion_Headless/Inputs\n",
        "!mkdir /content/drive/MyDrive/Face_Fusion_Headless/Outputs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pL88z9EXUJOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVHiNI-bb6IA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title RUN FACE FUSION\n",
        "\n",
        "#@markdown NOTE: You need to Execute the Mount Google Drive Cell before checking the Google_Drive Option\n",
        "Google_Drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "Target = \"Target.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "Source_Image = \"Source.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "Output = \"Output.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "if Google_Drive:\n",
        "  Target = f\"/content/drive/MyDrive/Face_Fusion_Headless/Inputs/{Target}\"\n",
        "  Source_Image = f\"/content/drive/MyDrive/Face_Fusion_Headless/Inputs/{Source_Image}\"\n",
        "  Output = f\"/content/drive/MyDrive/Face_Fusion_Headless/Outputs/{Output}\"\n",
        "\n",
        "Additional_Options = \"--face-selector-mode many --temp-frame-quality 100 --output-video-quality 100 --output-image-quality 100 --execution-providers azure cpu --execution-thread-count 8 --execution-queue-count 1 --face-enhancer-blend 100 --frame-enhancer-blend 100 --lip-syncer-model wav2lip_gan --frame-processors face_swapper face_enhancer frame_enhancer\" #@param {type:\"string\"}\n",
        "\n",
        "%cd \"/content/y8e23w6s/\"\n",
        "!python run.py --headless -t \"{Target}\" -s \"{Source_Image}\" -o \"{Output}\" $Additional_Options"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
