{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nick088Official/FaceFusion-Colab-HF-Space/blob/main/FaceFusion_Headless_No_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVreYca3LcQ"
      },
      "source": [
        "# **FACEFUSION NO UI (Manual, Headless Version) 2.5.1**\n",
        "\n",
        "DeepFake AI Tool\n",
        "\n",
        "Credits: [Nick088](https://linktr.ee/Nick088) (making the no ui one and now taking support unofficially of FaceFusion colabs), Lusbert (Fixing the Encoding Output Deepfake), [Original FaceFusion team](https://github.com/facefusion/facefusion) (for making the program and the old ui colab before they stopped giving support)\n",
        "\n",
        "## GUIDE:\n",
        "1. Run Install FaceFusion cell and wait until it finishes.\n",
        "\n",
        "Remember for the rest of the guide:\n",
        "\n",
        "Source = The photo used as a reference point\n",
        "\n",
        "Target = The video or photo whose face you want to change\n",
        "\n",
        "Inputs = Target & Source\n",
        "\n",
        "Output = Final Results (an image if the target is one, or a video if the target is one\n",
        "\n",
        "2. Now you got 2 ways to upload the inputs:\n",
        "\n",
        "A) MANUAL INPUTS WAY:\n",
        "1. Run the 'Upload Target Vid/Pic & Source Pic' cell and upload both of them (either do ctrl and select both of them or just select one, then run it again, and select the other)\n",
        "\n",
        "2. Wait until the Target and Source files are loaded, and then in the Run Face Fusion cell, in Target insert the name of the Target Video/Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), for Source_Image put the name of the Photo Source, and for Output insert the name (with the same extension of the Target) you want to give to the final output.\n",
        "\n",
        "B) GOOGLE DRIVE INPUTS WAY:\n",
        "1. Run the Mount Google Drive Cell\n",
        "\n",
        "2. Run the 'Upload Target Vid/Pic & Source Pic' cell and upload both of them (either do ctrl and select both of them or just select one, then run it again, and select the other)\n",
        "\n",
        "3. Wait until the Target and Source files are loaded, and then in the Run Face Fusion cell, in Target_Video insert the name of the Video or the Target Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), for Source_Image put the name of the photo Source, and for Output insert the name you want to give to the final output (with the same extension of the Target).\n",
        "\n",
        "Now the rest of the guide works for both ways regardless which you used:\n",
        "\n",
        "4. You can put additional options in the Run Face Fusion Cell before running it, all options with explainations:\n",
        "```\n",
        "misc:\n",
        "  --log-level {error,warn,info,debug}                                                                                    adjust the message severity displayed in the terminal\n",
        "execution:\n",
        "  --execution-providers EXECUTION_PROVIDERS [EXECUTION_PROVIDERS ...]                                                    accelerate the model inference using different providers (choices: tensorrt, cuda, azure, cpu)\n",
        "  --execution-thread-count [1-128]                                                                                       specify the amount of parallel threads while processing\n",
        "  --execution-queue-count [1-32]                                                                                         specify the amount of frames each thread is processing\n",
        "memory:\n",
        "  --video-memory-strategy {strict,moderate,tolerant}                                                                     balance fast frame processing and low vram usage\n",
        "  --system-memory-limit [0-128]                                                                                          limit the available ram that can be used while processing\n",
        "face analyser:\n",
        "  --face-analyser-order {left-right,right-left,top-bottom,bottom-top,small-large,large-small,best-worst,worst-best}      specify the order in which the face analyser detects faces.\n",
        "  --face-analyser-age {child,teen,adult,senior}                                                                          filter the detected faces based on their age\n",
        "  --face-analyser-gender {female,male}                                                                                   filter the detected faces based on their gender\n",
        "  --face-detector-model {many,retinaface,scrfd,yoloface,yunet}                                                           choose the model responsible for detecting the face\n",
        "  --face-detector-size FACE_DETECTOR_SIZE                                                                                specify the size of the frame provided to the face detector\n",
        "  --face-detector-score [0.0-1.0]                                                                                        filter the detected faces base on the confidence score\n",
        "  --face-landmarker-score [0.0-1.0]                                                                                      filter the detected landmarks base on the confidence score\n",
        "face selector:\n",
        "  --face-selector-mode {many,one,reference}                                                                              use reference based tracking or simple matching\n",
        "  --reference-face-position REFERENCE_FACE_POSITION                                                                      specify the position used to create the reference face\n",
        "  --reference-face-distance [0.0-1.5]                                                                                    specify the desired similarity between the reference face and target face\n",
        "  --reference-frame-number REFERENCE_FRAME_NUMBER                                                                        specify the frame used to create the reference face\n",
        "face mask:\n",
        "  --face-mask-types FACE_MASK_TYPES [FACE_MASK_TYPES ...]                                                                mix and match different face mask types (choices: box, occlusion, region)\n",
        "  --face-mask-blur [0.0-1.0]                                                                                             specify the degree of blur applied the box mask\n",
        "  --face-mask-padding FACE_MASK_PADDING [FACE_MASK_PADDING ...]                                                          apply top, right, bottom and left padding to the box mask\n",
        "  --face-mask-regions FACE_MASK_REGIONS [FACE_MASK_REGIONS ...]                                                          choose the facial features used for the region mask (choices: skin, left-eyebrow, right-eyebrow, left-eye, right-eye, eye-glasses, nose, mouth, upper-lip, lower-lip)\n",
        "frame extraction:\n",
        "  --trim-frame-start TRIM_FRAME_START                                                                                    specify the the start frame of the target video\n",
        "  --trim-frame-end TRIM_FRAME_END                                                                                        specify the the end frame of the target video\n",
        "  --temp-frame-format {bmp,jpg,png}                                                                                      specify the temporary resources format\n",
        "  --keep-temp                                                                                                            keep the temporary resources after processing\n",
        "output creation:\n",
        "  --output-image-quality [0-100]                                                                                         specify the image quality which translates to the compression factor\n",
        "  --output-image-resolution OUTPUT_IMAGE_RESOLUTION                                                                      specify the image output resolution based on the target image\n",
        "  --output-video-encoder {libx264,libx265,libvpx-vp9,h264_nvenc,hevc_nvenc,h264_amf,hevc_amf}                            specify the encoder use for the video compression\n",
        "  --output-video-preset {ultrafast,superfast,veryfast,faster,fast,medium,slow,slower,veryslow}                           balance fast video processing and video file size\n",
        "  --output-video-quality [0-100]                                                                                         specify the video quality which translates to the compression factor\n",
        "  --output-video-resolution OUTPUT_VIDEO_RESOLUTION                                                                      specify the video output resolution based on the target video\n",
        "  --output-video-fps OUTPUT_VIDEO_FPS                                                                                    specify the video output fps based on the target video\n",
        "  --skip-audio                                                                                                           omit the audio from the target video\n",
        "frame processors:\n",
        "  --frame-processors FRAME_PROCESSORS [FRAME_PROCESSORS ...]                                                                                            load a single or multiple frame processors. (choices: face_debugger, face_enhancer, face_swapper, frame_colorizer, frame_enhancer, lip_syncer, ...)\n",
        "  --face-debugger-items FACE_DEBUGGER_ITEMS [FACE_DEBUGGER_ITEMS ...]                                                                                   load a single or multiple frame processors (choices: bounding-box, face-landmark-5, face-landmark-5/68, face-landmark-68, face-landmark-68/5, face-mask, face-detector-score, face-landmarker-score, age, gender)\n",
        "  --face-enhancer-model {codeformer,gfpgan_1.2,gfpgan_1.3,gfpgan_1.4,gpen_bfr_256,gpen_bfr_512,gpen_bfr_1024,gpen_bfr_2048,restoreformer_plus_plus}     choose the model responsible for enhancing the face\n",
        "  --face-enhancer-blend [0-100]                                                                                                                         blend the enhanced into the previous face\n",
        "  --face-swapper-model {blendswap_256,inswapper_128,inswapper_128_fp16,simswap_256,simswap_512_unofficial,uniface_256}                                  choose the model responsible for swapping the face\n",
        "  --frame-colorizer-model {ddcolor,ddcolor_artistic,deoldify,deoldify_artistic,deoldify_stable}                                                                                  choose the model responsible for colorizing the frame\n",
        "  --frame-colorizer-blend [0-100]                                                                                                                       blend the colorized into the previous frame\n",
        "  --frame-enhancer-model {lsdir_x4,nomos8k_sc_x4,real_esrgan_x2,real_esrgan_x2_fp16,real_esrgan_x4,real_esrgan_x4_fp16,real_hatgan_x4,span_kendata_x4}  choose the model responsible for enhancing the frame\n",
        "  --frame-enhancer-blend [0-100]                                                                                                                        blend the enhanced into the previous frame\n",
        "  --lip-syncer-model {wav2lip_gan}                                                                                                                      choose the model responsible for syncing the lips\n",
        "```\n",
        "\n",
        "5. Run the Download Output Result cell.\n",
        "\n",
        "6. Run the Delete Inputs & Outputs cell for clearing up space, you may want to do this especially if you mount your Google Drive, or can just skip this if for example you may need to use again one of the inputs, it's your choice.\n",
        "\n",
        "\n",
        "**TIPS:**\n",
        "- Be careful to make some links every now and then so it doesn't disconnect due to inactivity, you could also check https://rentry.org/colab_workarounds.\n",
        "\n",
        "- If the process is too slow you can speed it up at the cost of losing some quality of the output, by removing “face_enhancer” and “frame_enhancer” in Options of the Run Face Fusion Cell,after frame_processors, before running it again.\n",
        "\n",
        "- If the process is too slow, you can split the Target video into videos of 1 minute each, and run the process again for each video, then finally link them all together by placing them on any free editing app like Capcut\n",
        "\n",
        "## CHANGELOG\n",
        "### Update - April 16th, 2024\n",
        "Updated facefusion from 2.5.0 to 2.5.1\n",
        "### Update - April 13th, 2024\n",
        "Updated facefusion from 2.4.1 to 2.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlrnUA3i3gMB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install FaceFusion\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  !apt-get install nvidia-cuda-toolkit\n",
        "  print(\"Using GPU\")\n",
        "else:\n",
        "  print(\"Using CPU\")\n",
        "\n",
        "!git clone https://tinyurl.com/3dyrb6e7 --branch 2.5.1 --single-branch\n",
        "%cd /content/3dyrb6e7\n",
        "!python install.py --onnxruntime cuda-11.8 --skip-conda\n",
        "\n",
        "Drive_Is_Mounted = False\n",
        "\n",
        "clear_output()\n",
        "print(\"Installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (OPTIONAL) Mount Google Drive\n",
        "\n",
        "#@markdown NOTE: If you run this, the rest of the colab will use your Google Drive inputs and outputs folder instead of the Google Colab ones.\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the parent directory path\n",
        "parent_dir = '/content/drive/MyDrive/FaceFusion_Headless'\n",
        "\n",
        "# Create the parent directory if it doesn't exist\n",
        "if not os.path.exists(parent_dir):\n",
        "    os.mkdir(parent_dir)\n",
        "\n",
        "# Create the 'Inputs' and 'Outputs' subdirectories\n",
        "inputs_dir = os.path.join(parent_dir, 'Inputs')\n",
        "outputs_dir = os.path.join(parent_dir, 'Outputs')\n",
        "\n",
        "if not os.path.exists(inputs_dir):\n",
        "  os.mkdir(inputs_dir)\n",
        "\n",
        "if not os.path.exists(outputs_dir):\n",
        "  os.mkdir(outputs_dir)\n",
        "\n",
        "Drive_Is_Mounted = True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pL88z9EXUJOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Target Vid/Pic & Source Pic\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will upload the inputs into Google Drive.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "%cd /content/3dyrb6e7\n",
        "# upload\n",
        "uploaded = files.upload()\n",
        "\n",
        "if Drive_Is_Mounted:\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    target_directory = '/content/drive/MyDrive/FaceFusion_Headless/Inputs'\n",
        "\n",
        "    # Get a list of all files uploaded by the user\n",
        "    user_uploaded_files = list(uploaded.keys())\n",
        "\n",
        "    # Move each user-uploaded file to the target directory\n",
        "    for file_name in user_uploaded_files:\n",
        "        source_path = os.path.join('/content/3dyrb6e7', file_name)\n",
        "        target_path = os.path.join(target_directory, file_name)\n",
        "        shutil.move(source_path, target_path)\n",
        "        print(f\"Moved '{file_name}' to Google Drive FaceFusion_Headless/Inputs folder.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2nQlsNkziZ2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVHiNI-bb6IA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run FaceFusion\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will use the inputs from Google Drive and upload the outputs there.\n",
        "\n",
        "Target = \"Target.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "Source_Image = \"Source_Image.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "Output_Path = \"/content/3dyrb6e7/\"\n",
        "\n",
        "Output = \"Output.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "if Drive_Is_Mounted:\n",
        "  Target = f\"/content/drive/MyDrive/FaceFusion_Headless/Inputs/{Target}\"\n",
        "  Source_Image = f\"/content/drive/MyDrive/FaceFusion_Headless/Inputs/{Source_Image}\"\n",
        "  Output = f\"/content/drive/MyDrive/FaceFusion_Headless/Outputs/{Output}\"\n",
        "else:\n",
        "  Output = f\"{Output_Path}{Output}\"\n",
        "\n",
        "Additional_Options = \"--face-selector-mode many --output-video-quality 100 --output-image-quality 100 --execution-providers cpu cuda --execution-thread-count 8 --execution-queue-count 1 --face-enhancer-blend 100 --frame-enhancer-blend 100 --lip-syncer-model wav2lip_gan --frame-processors face_swapper face_enhancer frame_enhancer\" #@param {type:\"string\"}\n",
        "\n",
        "%cd \"/content/3dyrb6e7/\"\n",
        "\n",
        "run = f\"run.py --headless -t '{Target}' -s '{Source_Image}' -o '{Output}' {Additional_Options}\"\n",
        "\n",
        "!python $run\n",
        "\n",
        "Output_Is_Video = False\n",
        "\n",
        "# check if the output is a video cus it has weird encoding\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def is_video_file(file_path):\n",
        "    video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.flv', '.wmv', '.webm', '.vob']\n",
        "    file_extension = Path(file_path).suffix.lower()\n",
        "    return file_extension in video_extensions\n",
        "\n",
        "if is_video_file(Output):\n",
        "  print(f\"{Output} is a video file, going to encode it in the right way as in after the 2.3.0 version it encodes it weird.\")\n",
        "  Output_Is_Video = True\n",
        "else:\n",
        "  print(f\"{Output} is an image file, you are can download it by clicking the cell below!\")\n",
        "\n",
        "\n",
        "\n",
        "# FIX the encoding as someway after the 2.3.0 update it got kinda silly\n",
        "\n",
        "if Output_Is_Video:\n",
        "  import os\n",
        "  !pip install ffmpeg\n",
        "  import ffmpeg\n",
        "\n",
        "  input_path = f\"{Output}\"\n",
        "  output_path = f'{Output}_final'\n",
        "\n",
        "  # Run FFmpeg command to encode the video the right way\n",
        "  fix = f\"ffmpeg -y -i '{input_path}' -c:v libx264 -preset medium -crf 16 -pix_fmt yuv420p -vf 'scale=trunc(iw/2)*2:trunc(ih/2)*2' -map 0:v:0? -c:a aac -map 0:a? -c:s mov_text -map 0:s? -map_chapters 0 -map_metadata 0 -f mp4 -threads 0 '{output_path}'\" #sligtly worse quality as it changes the encoding for it to work\n",
        "\n",
        "  !$fix\n",
        "  os.remove(input_path) # remove the weird encoded file\n",
        "  if output_path.endswith(\"_final\"):\n",
        "    new_filename = output_path.replace(\"_final\", \"\")\n",
        "    old_path = os.path.join(output_path)\n",
        "    new_path = os.path.join(new_filename)\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"Fixed the output video encoding, your video is all ready to download with the cell below!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dowload Output Result\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will download automatically the output from Google Drive FaceFusion_Headless/Outputs folder.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(f\"{Output}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EyM2ZChjrUNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Delete Inputs & Ouputs\n",
        "\n",
        "#@markdown NOTE: You may wanna do this just to free up space, especially if you mounted Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "os.remove(Output)\n",
        "os.remove(Target)\n",
        "os.remove(Source_Image)\n",
        "\n",
        "print(\"Succesfully deleted Inputs & Outputs!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lkjo075fcPW2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}