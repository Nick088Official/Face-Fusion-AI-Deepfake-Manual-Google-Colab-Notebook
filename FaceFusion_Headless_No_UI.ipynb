{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nick088Official/FaceFusion-Colab/blob/main/FaceFusion_Headless_No_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVreYca3LcQ"
      },
      "source": [
        "# **FACEFUSION NO UI (Manual, Headless Version) 2.4.1**\n",
        "\n",
        "DeepFake AI Tool\n",
        "\n",
        "Google Colab Notebook Made By [Nick088](https://linktr.ee/Nick088)\n",
        "\n",
        "## GUIDE:\n",
        "1. Run Install Face Fusion and wait until it finishes.\n",
        "\n",
        "Remember for the rest of the guide:\n",
        "\n",
        "Source = The photo used as a reference point\n",
        "\n",
        "Target = The video or photo whose face you want to change\n",
        "\n",
        "Inputs = Target & Source\n",
        "\n",
        "Output = Final Results (an image if the target is one, or a video if the target is one\n",
        "\n",
        "2. Now you got 2 ways to upload the inputs:\n",
        "\n",
        "A) MANUAL INPUTS WAY:\n",
        "1. Run the 'Upload Target Vid/Pic & Source Pic' cell and upload both of them (either do ctrl and select both of them or just select one, then run it again, and select the other)\n",
        "\n",
        "2. Wait until the Target and Source files are loaded, and then in the Run Face Fusion cell, in Target insert the name of the Target Video/Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), for Source_Image put the name of the Photo Source, and for Output insert the name (with the same extension of the Target) you want to give to the final output.\n",
        "\n",
        "B) GOOGLE DRIVE INPUTS WAY:\n",
        "1. Run the Mount Google Drive Cell\n",
        "\n",
        "2. Goto https://drive.google.com and go into the FaceFusion_Headless Folder, Upload your Image Source & Target Video/Photo in the Inputs folder inside of it, wait until the Target and Source files are fully loaded in it.\n",
        "\n",
        "3. Check the Google Drive Option in Run Face Fusion Cell, in Target_Video insert the name of the Video or the Target Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), for Source_Image put the name of the photo Source, and for Output insert the name you want to give to the final output (with the same extension of the Target).\n",
        "\n",
        "Now the rest of the guide works for both ways regardless which you used:\n",
        "\n",
        "4. You can put additional options in the Run Face Fusion Cell before running it, all options with explainations:\n",
        "```\n",
        "misc:\n",
        "  --skip-download                                                                                                        omit automate downloads and remote lookups\n",
        "  --log-level {error,warn,info,debug}                                                                                    adjust the message severity displayed in the terminal\n",
        "execution:\n",
        "  --execution-providers EXECUTION_PROVIDERS [EXECUTION_PROVIDERS ...]                                                    accelerate the model inference using different providers (choices: cpu, ...)\n",
        "  --execution-thread-count [1-128]                                                                                       specify the amount of parallel threads while processing\n",
        "  --execution-queue-count [1-32]                                                                                         specify the amount of frames each thread is processing\n",
        "memory:\n",
        "  --video-memory-strategy {strict,moderate,tolerant}                                                                     balance fast frame processing and low vram usage\n",
        "  --system-memory-limit [0-128]                                                                                          limit the available ram that can be used while processing\n",
        "face analyser:\n",
        "  --face-analyser-order {left-right,right-left,top-bottom,bottom-top,small-large,large-small,best-worst,worst-best}      specify the order in which the face analyser detects faces.\n",
        "  --face-analyser-age {child,teen,adult,senior}                                                                          filter the detected faces based on their age\n",
        "  --face-analyser-gender {female,male}                                                                                   filter the detected faces based on their gender\n",
        "  --face-detector-model {many,retinaface,scrfd,yoloface,yunet}                                                           choose the model responsible for detecting the face\n",
        "  --face-detector-size FACE_DETECTOR_SIZE                                                                                specify the size of the frame provided to the face detector\n",
        "  --face-detector-score [0.0-1.0]                                                                                        filter the detected faces base on the confidence score\n",
        "  --face-landmarker-score [0.0-1.0]                                                                                      filter the detected landmarks base on the confidence score\n",
        "face selector:\n",
        "  --face-selector-mode {many,one,reference}                                                                              use reference based tracking or simple matching\n",
        "  --reference-face-position REFERENCE_FACE_POSITION                                                                      specify the position used to create the reference face\n",
        "  --reference-face-distance [0.0-1.5]                                                                                    specify the desired similarity between the reference face and target face\n",
        "  --reference-frame-number REFERENCE_FRAME_NUMBER                                                                        specify the frame used to create the reference face\n",
        "face mask:\n",
        "  --face-mask-types FACE_MASK_TYPES [FACE_MASK_TYPES ...]                                                                mix and match different face mask types (choices: box, occlusion, region)\n",
        "  --face-mask-blur [0.0-1.0]                                                                                             specify the degree of blur applied the box mask\n",
        "  --face-mask-padding FACE_MASK_PADDING [FACE_MASK_PADDING ...]                                                          apply top, right, bottom and left padding to the box mask\n",
        "  --face-mask-regions FACE_MASK_REGIONS [FACE_MASK_REGIONS ...]                                                          choose the facial features used for the region mask (choices: skin, left-eyebrow, right-eyebrow, left-eye, right-eye, eye-glasses, nose, mouth, upper-lip, lower-lip)\n",
        "frame extraction:\n",
        "  --trim-frame-start TRIM_FRAME_START                                                                                    specify the the start frame of the target video\n",
        "  --trim-frame-end TRIM_FRAME_END                                                                                        specify the the end frame of the target video\n",
        "  --temp-frame-format {bmp,jpg,png}                                                                                      specify the temporary resources format\n",
        "  --keep-temp                                                                                                            keep the temporary resources after processing\n",
        "output creation:\n",
        "  --output-image-quality [0-100]                                                                                         specify the image quality which translates to the compression factor\n",
        "  --output-image-resolution OUTPUT_IMAGE_RESOLUTION                                                                      specify the image output resolution based on the target image\n",
        "  --output-video-encoder {libx264,libx265,libvpx-vp9,h264_nvenc,hevc_nvenc,h264_amf,hevc_amf}                            specify the encoder use for the video compression\n",
        "  --output-video-preset {ultrafast,superfast,veryfast,faster,fast,medium,slow,slower,veryslow}                           balance fast video processing and video file size\n",
        "  --output-video-quality [0-100]                                                                                         specify the video quality which translates to the compression factor\n",
        "  --output-video-resolution OUTPUT_VIDEO_RESOLUTION                                                                      specify the video output resolution based on the target video\n",
        "  --output-video-fps OUTPUT_VIDEO_FPS                                                                                    specify the video output fps based on the target video\n",
        "  --skip-audio                                                                                                           omit the audio from the target video\n",
        "frame processors:\n",
        "  --frame-processors FRAME_PROCESSORS [FRAME_PROCESSORS ...]                                                             load a single or multiple frame processors. (choices: face_debugger, face_enhancer, face_swapper, frame_enhancer, lip_syncer, ...)\n",
        "  --face-debugger-items FACE_DEBUGGER_ITEMS [FACE_DEBUGGER_ITEMS ...]                                                    load a single or multiple frame processors (choices: bounding-box, face-landmark-5, face-landmark-5/68, face-landmark-68, face-mask, face-detector-score, face-landmarker-score, age, gender)\n",
        "  --face-enhancer-model {codeformer,gfpgan_1.2,gfpgan_1.3,gfpgan_1.4,gpen_bfr_256,gpen_bfr_512,restoreformer_plus_plus}  choose the model responsible for enhancing the face\n",
        "  --face-enhancer-blend [0-100]                                                                                          blend the enhanced into the previous face\n",
        "  --face-swapper-model {blendswap_256,inswapper_128,inswapper_128_fp16,simswap_256,simswap_512_unofficial,uniface_256}   choose the model responsible for swapping the face\n",
        "  --frame-enhancer-model {lsdir_x4,nomos8k_sc_x4,real_esrgan_x4,real_esrgan_x4_fp16,span_kendata_x4}                     choose the model responsible for enhancing the frame\n",
        "  --frame-enhancer-blend [0-100]                                                                                         blend the enhanced into the previous frame\n",
        "  --lip-syncer-model {wav2lip_gan}                                                                                       choose the model responsible for syncing the lips\n",
        "```\n",
        "\n",
        "5. Now you got 2 ways to get the outputs:\n",
        "\n",
        "A) MANUAL OUTPUTS WAY:\n",
        "1. (OPTIONAL) Run the Display the Output cell.\n",
        "\n",
        "2. Run the Download Output Result cell\n",
        "\n",
        "B) GOOGLE DRIVE OUTPUTS WAY:\n",
        "1. (OPTIONAL) Run the Display the Output cell.\n",
        "\n",
        "2. Run the Download Output Result cell\n",
        "\n",
        "3. REMINDER: The outputs and inputs are still saved on your Google Drive FaceFusion_Headless folder, you can also download it from there if you want, and may want to delete them after your done to free up some space\n",
        "\n",
        "**TIPS:**\n",
        "- Be careful to make some links every now and then so it doesn't disconnect due to inactivity, you could also check https://rentry.org/colab_workarounds.\n",
        "\n",
        "- If the process is too slow you can speed it up at the cost of losing some quality of the output, by removing “face_enhancer” and “frame_enhancer” in Options of the Run Face Fusion Cell,after frame_processors, before running it again.\n",
        "\n",
        "- If the process is too slow, you can split the Target video into videos of 1 minute each, and run the process again for each video, then finally link them all together by placing them on any free editing app like Capcut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlrnUA3i3gMB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install FaceFusion\n",
        "!apt-get install nvidia-cuda-toolkit\n",
        "!git clone https://tinyurl.com/3dyrb6e7 --branch 2.4.1 --single-branch\n",
        "%cd /content/3dyrb6e7\n",
        "!python install.py --onnxruntime cuda-11.8 --skip-venv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (OPTIONAL) Mount Google Drive\n",
        "\n",
        "#@markdown NOTE: If you run this, the rest of the colab will use your Google Drive inputs and outputs folder instead of the Google Colab ones.\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/drive')\n",
        "!mkdir /content/drive/MyDrive/FaceFusion_Headless/Inputs\n",
        "!mkdir /content/drive/MyDrive/FaceFusion_Headless/Outputs\n",
        "drive_is_mounted = True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pL88z9EXUJOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Target Vid/Pic & Source Pic\n",
        "\n",
        "#@markdown NOTE: NOTE: If you Runned the Mount Google Drive Cell, it will upload the inputs into Google Drive.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "%cd /content/3dyrb6e7\n",
        "# upload\n",
        "uploaded = files.upload()\n",
        "\n",
        "if drive_is_mounted:\n",
        "  import os\n",
        "  import shutil\n",
        "\n",
        "  # Specify the source directory (Colab file explorer)\n",
        "  source_directory = '/content/3dyrb6e7'\n",
        "\n",
        "  # Specify the target directory (Google Drive)\n",
        "  target_directory = '/content/drive/MyDrive/FaceFusion_Headless/Inputs'\n",
        "\n",
        "  # Get a list of all files in the source directory\n",
        "  file_list = os.listdir(source_directory)\n",
        "\n",
        "  # Move each file to the target directory\n",
        "  for file_name in file_list:\n",
        "      source_path = os.path.join(source_directory, file_name)\n",
        "      target_path = os.path.join(target_directory, file_name)\n",
        "      shutil.move(source_path, target_path)\n",
        "      print(f\"Moved '{file_name}' to Google Drive FaceFusion_Headless/Inputs folder.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2nQlsNkziZ2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVHiNI-bb6IA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run FaceFusion\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will use the inputs from Google Drive and upload the outputs there.\n",
        "\n",
        "Target = \"Target.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "Source_Image = \"Source_Image.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "Output_Path = \"/content/3dyrb6e7/\"\n",
        "\n",
        "Output = \"Output.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "Output_Final = f\"{Output_Path}{Output}\"\n",
        "\n",
        "if drive_is_mounted:\n",
        "  Target = f\"/content/drive/MyDrive/FaceFusion_Headless/Inputs/{Target}\"\n",
        "  Source_Image = f\"/content/drive/MyDrive/FaceFusion_Headless/Inputs/{Source_Image}\"\n",
        "  Output = f\"/content/drive/MyDrive/FaceFusion_Headless/Outputs/{Output}\"\n",
        "\n",
        "Additional_Options = \"--face-selector-mode many --output-video-quality 100 --output-image-quality 100 --execution-providers azure cpu --execution-thread-count 8 --execution-queue-count 1 --face-enhancer-blend 100 --frame-enhancer-blend 100 --lip-syncer-model wav2lip_gan --frame-processors face_swapper frame_enhancer face_enhancer\" #@param {type:\"string\"}\n",
        "\n",
        "%cd \"/content/3dyrb6e7/\"\n",
        "\n",
        "run = f\"run.py --headless -t '{Target}' -s '{Source_Image}' -o '{Output_Final}' {Additional_Options}\"\n",
        "\n",
        "!python $run\n",
        "\n",
        "# FIX the encoding as someway after the 2.3.0 update it got kinda silly\n",
        "import os\n",
        "!pip install ffmpeg\n",
        "import ffmpeg\n",
        "\n",
        "\n",
        "if drive_is_mounted:\n",
        "  input_path = f\"{Output}\"\n",
        "  output_path = f'{Output}'\n",
        "else:\n",
        "  input_path = f\"{Output_Final}\"\n",
        "  output_path = f'{Output_Final}'\n",
        "\n",
        "# Run FFmpeg command to encode the video\n",
        "fix = f\"ffmpeg -y -i '{input_path}' -c:v copy -map 0:v:0? -c:a copy -map 0:a? -c:s copy -sn -map_chapters 0 -map_metadata 0 -f mp4 -threads 0 '{output_path}'\" #sligtly worse quality as it changes the encoding for it to work\n",
        "\n",
        "!$fix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display the Output\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will show the output from Google Drive FaceFusion_Headless/Outputs folder.\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Read the video file (replace 'video.mp4' with your actual video file path)\n",
        "mp4 = open(f'{Output}' if drive_is_mounted else f'{Output_Final}', 'rb').read()\n",
        "\n",
        "# Create a data URL for the video\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "# Display the video\n",
        "HTML(f\"\"\"\n",
        "<video width=400 controls>\n",
        "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w0XARzyEuKSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dowload Output Result\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will download automatically the output from Google Drive FaceFusion_Headless/Outputs folder.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "if Google_Drive:\n",
        "  files.download(f\"{Output}\")\n",
        "else:\n",
        "  files.download(f\"{Output_Final}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EyM2ZChjrUNH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}